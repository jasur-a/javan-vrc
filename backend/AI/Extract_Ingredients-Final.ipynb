{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0015b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip  install xlrd nltk spacy pandas sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf71228",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab95de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install   pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f38964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_dep_news_trf\n",
    "#conda install -c conda-forge spacy-model-es_dep_news_trf\n",
    "#pip install xlrd\n",
    "#pip install Pattern\n",
    "\n",
    "# Librerías para PNL\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import es_dep_news_trf\n",
    "\n",
    "\n",
    "# Otras librerías\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pattern.es import pluralize\n",
    "from pattern.es import singularize\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6939c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_sm / es_core_news_lg\n",
    "#nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64fc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "Token.set_extension('need_punct', default=False, force=True)\n",
    "\n",
    "def parse_text(text, prev_doc = []):\n",
    "    parsed_text = []\n",
    "    \n",
    "    nlp = spacy.load(\"es_dep_news_trf\")\n",
    "    doc = nlp(text)\n",
    "      \n",
    "    for idx, token in enumerate(doc):\n",
    "        #if not token.like_url and not token.like_email:\n",
    "            description = prev_doc[idx][6] if len(prev_doc) > 0 else \"\"\n",
    "            punt = prev_doc[idx][7] if len(prev_doc) > 0 else token._.need_punct\n",
    "            parsed_text.append([token.text.lower(), token.lemma_, token.pos_, spacy.explain(token.pos_), token.dep_, description, punt, token.morph.get(\"Gender\"), token.morph.get('Number')])\n",
    "\n",
    "    return parsed_text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60d0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_words(parsed_text):\n",
    "    tokenized_words = []\n",
    "    \n",
    "    stopwords = [\".\", \",\", \"mi\", \"hola\", \"mmm\", \"gracias\", \"\\n\", \" \"]\n",
    "\n",
    "    no_stopwords = [word for word in parsed_text if word[0] not in stopwords]\n",
    "\n",
    "    for index, word in enumerate(no_stopwords):\n",
    "        word.insert(0, index)\n",
    "        tokenized_words.append(word)\n",
    "        \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbfbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene solo el texto de las palabras\n",
    "\"\"\"\n",
    "def get_word_text(words):\n",
    "    return [word[1] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363483f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cuando hay ingredientes en la descripcion se obtiene solo este texto\n",
    "\"\"\"\n",
    "def get_text_from_description(parse_text):\n",
    "    from_description = []\n",
    "    paragraph = []\n",
    "    paragraphs = []\n",
    "    \n",
    "    for i, word in enumerate(parse_text):\n",
    "        if word[0] != \"descripción\" or parse_text[i + 1][0] != \"del\" or parse_text[i + 2][0] != \"video\":\n",
    "            continue\n",
    "        for w in range(i + 2, len(parse_text)):\n",
    "            if parse_text[w][0] == \"ingredientes\":\n",
    "                for j in range(w + 1, len(parse_text)):\n",
    "                    if parse_text[j][0] == \":\":\n",
    "                        continue\n",
    "                    if \"\\n\" in parse_text[j][0]:\n",
    "                        paragraphs.append(paragraph)\n",
    "                        paragraph = []\n",
    "                        continue\n",
    "                    paragraph.append(parse_text[j][0])\n",
    "                    #from_description.append(parse_text[j][0])\n",
    "     \n",
    "    # Eliminar parrafos inncessarios\n",
    "    copy_paragraphs = paragraphs.copy()\n",
    "    for index, p in enumerate(paragraphs):\n",
    "        p_tokens = nlp(\" \".join(p))\n",
    "        for token in p_tokens:\n",
    "            if token.like_url:\n",
    "                copy_paragraphs.pop(index)\n",
    "                break\n",
    "    \n",
    "    from_description = []\n",
    "    for pars in copy_paragraphs:\n",
    "        for p in pars:\n",
    "            from_description.append(p)\n",
    "        from_description.append(\"\\n\")\n",
    "\n",
    "    if len(from_description) == 0:\n",
    "        return []\n",
    "    else:  \n",
    "        return from_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6699976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define la palabra y la convierte a plural usando la función pluralize().\n",
    "\"\"\" \n",
    "def to_plural(word):\n",
    "    # #https://stackoverflow.com/questions/31387905/converting-plural-to-singular-in-a-text-file-with-python\n",
    "    # Installing NLTK data to import\n",
    "    # and run en module of pattern\n",
    "    #nltk.download('popular')\n",
    "\n",
    "    return pluralize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8cae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define la palabra y la convierte a singular usando la función singularize().\n",
    "\"\"\" \n",
    "def to_singular(word):\n",
    "    # # Import the NLTK module https://www.geeksforgeeks.org/python-program-to-convert-singular-to-plural/\n",
    "    return singularize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f488420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene los ingredientes del dataset.\n",
    "\"\"\"\n",
    "def get_ingredients_from_file(dataset):\n",
    "    return pd.read_excel(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc8befb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une los ingredientes con sus respectivos complementos descriptivos. #TODO eliminar\n",
    "def merge_ingredients_with_description(main_ingredient, complementary1, complementary2):\n",
    "    data = []\n",
    "    data = main_ingredient + (' ' + complementary1).fillna('') + (' ' + complementary2).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c66940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocesar información (se convierte la primera columna a minusculas y se eliminan NAN)\n",
    "\"\"\" \n",
    "def get_ingredients(main_ingredient):\n",
    "    basic_ingredients = main_ingredient.dropna().T.drop_duplicates().T\n",
    "\n",
    "    ingredients = []\n",
    "    for ing in basic_ingredients:\n",
    "        ingredients.append(to_plural(ing))\n",
    "        ingredients.append(to_singular(ing))\n",
    "        \n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d99eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los datos de un archivo json.\n",
    "\"\"\"\n",
    "def parse_json_file(jsonfile):\n",
    "    with open(jsonfile) as user_file:\n",
    "        measures_content = user_file.read()\n",
    "        parsed_json = json.loads(measures_content)\n",
    "    return parsed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1efbb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene las medidas del dataset.\n",
    "\"\"\" \n",
    "def get_measures_from_file(jsonfile):\n",
    "    measures_set = []\n",
    "    \n",
    "    parsed_json = parse_json_file(jsonfile)\n",
    "    \n",
    "    for liq_items in parsed_json['liquidos']:\n",
    "        measures_set.append(liq_items)\n",
    "\n",
    "    for sol_items in parsed_json['solido']:\n",
    "        measures_set.append(sol_items)\n",
    "    \n",
    "    return measures_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2ead00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clase que puede traducir cadenas de palabras numéricas \n",
    "comunes en espanol y convertirlas en la cantidad numérica correspondiente.\n",
    "\"\"\"\n",
    "class WordsToInt():\n",
    "    # Asignación de dígitos a nombres relativos a unidades \"ones\".\n",
    "    __ones__ = { 'un':   1, 'una':   1, 'uno':   1, 'once':     11,\n",
    "                 'dos':   2, 'doce':     12,\n",
    "                 'tres': 3, 'trece':   13,\n",
    "                 'cuatro':  4, 'catorce':   14,\n",
    "                 'cinco':  5, 'quince':    15,\n",
    "                 'seis':   6, 'dieciseis':    16,\n",
    "                 'siete': 7, 'diecisiete':  17,\n",
    "                 'ocho': 8, 'dieciocho':   18,\n",
    "                 'nueve':  9, 'diecinueve':   19,\n",
    "                 'diez': 10\n",
    "               }\n",
    "    \n",
    "    # Asignación de dígitos a nombres relativos 'decenas'.\n",
    "    __tens__ = { \n",
    "                 'veinte':  20,\n",
    "                 'treinta':  30,\n",
    "                 'cuarenta':   40,\n",
    "                 'cincuenta':   50,\n",
    "                 'sesenta':   60,\n",
    "                 'setenta': 70,\n",
    "                 'ochenta':  80,\n",
    "                 'noventa':  90 \n",
    "    }\n",
    "    \n",
    "    # Asignación de dígitos a nombres relativos 'centenas'.\n",
    "    __hundreds__ = {\n",
    "                'cien': 100, 'ciento': 100,\n",
    "                'quinientos': 500,\n",
    "                'setecientos': 700,\n",
    "                'mil':  1000,\n",
    "    }\n",
    "    \n",
    "    # Lista ordenada de los nombres asignados a los grupos de miles.\n",
    "    __groups__ = { \n",
    "                   'mil':  1000,\n",
    "                   'millon':   1000000,\n",
    "                   'billoon':   1000000000,\n",
    "                   'trillon':  1000000000000 \n",
    "    }\n",
    "    \n",
    "    # Asignación de dígitos a nombres relativos de 'fracciones'.\n",
    "    __fraction__ = { 'medio':  2, 'media': 2, 'tercio': 5, 'cuarto': 4, \"cuarta\": 5 }\n",
    "\n",
    "    # Expresión regular que busca nombres de grupos de números y captura:\n",
    "    # 1- la cadena que precede al nombre del grupo, y\n",
    "    # 2- el nombre del grupo (o una cadena vacía si el\n",
    "    # el valor capturado es simplemente el final de la cadena\n",
    "    # indicando el grupo 'unos', que normalmente es\n",
    "    # no expresado)\n",
    "    __groups_re__ = re.compile(\n",
    "        r'\\s?([\\w\\s]+?)(?:\\s((?:%s))|$)' %\n",
    "        ('|'.join(__groups__))\n",
    "        )\n",
    "\n",
    "    # Expresión regular que busca dentro de un solo grupo de números para\n",
    "    # 'n cien' y captura:\n",
    "    # 1- la cadena que precede de 'cientos', y\n",
    "    # 2- la cadena que sigue al 'cientos'.\n",
    "    # Se considerará como el número que indica el\n",
    "    # valor posicional de las decenas y las unidades del grupo.\n",
    "    __hundreds_re__ = re.compile(r'([\\w\\s]+)cientos(?:\\s(.*)|$)')\n",
    "    \n",
    "    __hundreds_extra__ = re.compile(r'([\\w]+)(?:\\s(.*)|$)')\n",
    "\n",
    "    # Expresión regular que se ve dentro de un solo número o\n",
    "    # grupo al que ya se le extrajo su valor de 'cientos'\n",
    "    # para un patrón de 'decenas' (es decir, 'cuarenta y dos') y captura:\n",
    "    #1- las decenas\n",
    "    #2- los unos\n",
    "    __tens_and_ones_re__ =  re.compile(\n",
    "        r'((?:%s))(?:\\s(.*)|$)' %\n",
    "        ('|'.join(__tens__.keys()))\n",
    "        )\n",
    "    \n",
    "    # Expresión regular que busca dentro de un solo grupo de números para\n",
    "    # 'fracción' y captura:\n",
    "    # 1- el primer número ordinario (1)\n",
    "    # 2- el nombre fraccionario (4)\n",
    "    # Al final da 1/4\n",
    "    __fraction_re__ = re.compile(\n",
    "        r'\\s?([\\w\\s]+?)(?:\\s((?:%s))|$)' %\n",
    "        ('|'.join(__fraction__))\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    Analiza las palabras hasta encontrar número que describen.\n",
    "    \"\"\"\n",
    "    def parse(self, words):\n",
    "        # Se van a analizar las palabras con ciertos tags o lemas.\n",
    "        nlp = spacy.load(\"es_dep_news_trf\")\n",
    "        \n",
    "        words = words.lower().strip()\n",
    "       \n",
    "        # Crea una lista para guardar los grupos de números tal como los encontramos dentro\n",
    "        # la cadena de palabras.\n",
    "        groups = {}        \n",
    "        # Crea una variable para guardar un número.\n",
    "        num = 0\n",
    "         \n",
    "        # Se dividen las palabras para saber si existe algun número en las listas numéricas.\n",
    "        split_words = words.split()\n",
    "        updated_words = \"\"\n",
    "        \n",
    "        indexes = []\n",
    "        found_words = []\n",
    "        firstNumber = []\n",
    "        \n",
    "        for idx, word in enumerate(split_words):\n",
    "            # Se tokeniza la palabra numérica.\n",
    "            doc = nlp(word)\n",
    "            \n",
    "            tokenized_number = [[ w.pos_, w.lemma_, w.text] for w in doc][0]\n",
    "            \n",
    "            # Número no puede ser igual al anterior\n",
    "            if idx > 0 and word == split_words[idx - 1]:\n",
    "                continue\n",
    "            \n",
    "            if tokenized_number[0] == \"NUM\" or (tokenized_number[2] == \"uno\" or tokenized_number[2] == \"un\" or tokenized_number[2] != \"unas\" and tokenized_number[2] == \"una\") or tokenized_number[1] == \"cuarto\":\n",
    "          #  if tokenized_number[0] == \"NUM\" or tokenized_number[2] == \"cuarto\" or (tokenized_number[2] == \"uno\" or tokenized_number[2] == \"un\" and tokenized_number[2] == \"una\"):\n",
    "                updated_words += \" \" + word\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Se crea nuevamente una cadena de texto más limpio.\n",
    "        updated_words = updated_words.strip()\n",
    "        \n",
    "        # En caso de que una palabra contenga una fracción.\n",
    "        has_fraction = [True for fr_word in split_words if fr_word in WordsToInt.__fraction__.keys()]\n",
    "        \n",
    "        if len(split_words) > 1 and len(has_fraction) > 0 and has_fraction[0]:\n",
    "            group_fraction = \"\"\n",
    "            for group in WordsToInt.__fraction_re__.findall(updated_words):\n",
    "                fraction_match = WordsToInt.__fraction_re__.match(group[0])\n",
    "            \n",
    "                if group[1] in WordsToInt.__fraction__:\n",
    "                    tens_and_ones = group[0]\n",
    "                    \n",
    "                    if group[0] in WordsToInt.__fraction__:\n",
    "                        return \"1/\" + str(WordsToInt.__fraction__[group[0]])\n",
    "                    \n",
    "                    if tens_and_ones not in WordsToInt.__ones__.keys():\n",
    "                        split_ones = tens_and_ones.split()\n",
    "                        tens_and_ones = split_ones[0]\n",
    "                \n",
    "                    if tens_and_ones in WordsToInt.__ones__.keys():\n",
    "                        group_fraction = str(WordsToInt.__ones__[tens_and_ones]) + \"/\" + str(WordsToInt.__fraction__[group[1]])\n",
    "                \n",
    "            return group_fraction\n",
    "            \n",
    "        # Para números enteros\n",
    "        for group in WordsToInt.__groups_re__.findall(updated_words):\n",
    "            # Determina la posición de este grupo de números\n",
    "            # dentro del número entero.\n",
    "            # Se asume que el índice de grupo es el grupo first/ones\n",
    "            # hasta que se determine que es un grupo superior.\n",
    "            group_multiplier = 1\n",
    "            group_num = 0\n",
    "            \n",
    "             # Determina el valor de este grupo de números\n",
    "            if group[1] in WordsToInt.__groups__:\n",
    "                group_multiplier = WordsToInt.__groups__[group[1]]\n",
    "           \n",
    "            # Crea una variable para guardar lo que queda cuando\n",
    "            # se eliminan las \"centenas\" (es decir, los valores de las decenas y las unidades)\n",
    "            hundreds_match = WordsToInt.__hundreds_re__.match(group[0])\n",
    "            \n",
    "            tens_and_ones = None\n",
    "            \n",
    "            # Se verifica si existe algun digito que coincida con un patrón de \"hundreds extra\".\n",
    "            hundreds_match1 = WordsToInt.__hundreds_extra__.match(group[0])\n",
    "            match_hundreds_extra = hundreds_match1.group(1) in WordsToInt.__hundreds__.keys()\n",
    "            \n",
    "            # En caso de que así sea:\n",
    "            if hundreds_match1 is not None and hundreds_match1.group(1) is not None and match_hundreds_extra:                           \n",
    "                # Se toma el valor del dígito\n",
    "                group_num = WordsToInt.__hundreds__[hundreds_match1.group(1)]\n",
    "                # Se guarda el valor posicional de las decenas y las unidades.\n",
    "                tens_and_ones = hundreds_match1.group(2)\n",
    "                       \n",
    "            # Si hay una cadena en este grupo que coincida con el patrón 'n cien'\n",
    "            elif hundreds_match is not None and hundreds_match.group(1) is not None:\n",
    "                # Multiplica el valor 'n' por 100 e incrementa el valor de este grupo\n",
    "                group_num = group_num + \\\n",
    "                            (WordsToInt.__ones__[hundreds_match.group(1)] * 100)\n",
    "                # Se guarda el valor posicional de las decenas y las unidades.\n",
    "                tens_and_ones = hundreds_match.group(2)\n",
    "            else:\n",
    "            # Si no hubiera ninguna cadena que coincidiera con el patrón 'n cien',\n",
    "            # supone que toda la cadena contiene solo decenas y unidades\n",
    "            # como valores posicionales.\n",
    "                tens_and_ones = group[0]\n",
    "            # Si la cadena de 'decenas y unidades' está vacía, se pasa al siguiente grupo\n",
    "            if tens_and_ones is None:\n",
    "                # Incrementa el número total con el número de grupo actual * su multiplicador\n",
    "                num = num + (group_num * group_multiplier)\n",
    "                continue\n",
    "           # Busca las decenas y las unidades \n",
    "            tn1_match = WordsToInt.__tens_and_ones_re__.match(tens_and_ones)\n",
    "            # Si el patrón coincide, hay un valor posicional de 'decenas'\n",
    "            if tn1_match is not None:\n",
    "                # Agrega las decenas\n",
    "                group_num = group_num + WordsToInt.__tens__[tn1_match.group(1)]\n",
    "                 # Agrega las unidades\n",
    "                if tn1_match.group(2) is not None:\n",
    "                    group_num = group_num + WordsToInt.__ones__[tn1_match.group(2)] \n",
    "            else:\n",
    "            # Asume que las 'decenas y unidades' en realidad solo contenían las unidade.\n",
    "                if tens_and_ones not in WordsToInt.__ones__.keys():\n",
    "                    split_ones = tens_and_ones.split()\n",
    "                    tens_and_ones = split_ones[0]\n",
    "            \n",
    "            if tens_and_ones in WordsToInt.__ones__.keys():\n",
    "                group_num = group_num + WordsToInt.__ones__[tens_and_ones]\n",
    "                \n",
    "            # Incrementa el número total con el número de grupo actual * su multiplicador\n",
    "            num = num + (group_num * group_multiplier)\n",
    "        return num   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f6c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los vecinos anteriores y posteriores de una palabra,\n",
    "de acuerdo a una distancia dada.\n",
    "\"\"\"\n",
    "def wsd_caracteristicas_colocacion(context, instance, pos, dist=2, punt = []): #TODO CAMBIADO\n",
    "    features = {}\n",
    "    con = context\n",
    "    # la coma esta antes\n",
    "    punt_prev = True if pos - 1 in punt else None \n",
    "    #la coma esta despues\n",
    "    punt_next = True if pos in punt else None\n",
    "\n",
    "    # Las palabras serán almacenadas de acuerdo a su posición.\n",
    "    prev_words = []\n",
    "    next_words = []\n",
    "\n",
    "    # A partir de una posición dada, se obtienen las palabras previas requeridas\n",
    "    # dependiendo la distancia.\n",
    "    if not punt_prev:\n",
    "        for i in range(max(0, pos-dist), pos):\n",
    "            prev_words.append(con[i])\n",
    "        features[\"previous\"] = (' '.join(prev_words))\n",
    "    else:\n",
    "        features[\"previous\"] = ('')\n",
    "\n",
    "     # A partir de una posición dada, se obtienen las palabras posteriores requeridas\n",
    "    # dependiendo la distancia.\n",
    "    if not punt_next:\n",
    "        for i in range(pos+1, min(pos+dist+1, len(con))):\n",
    "            next_words.append(con[i])\n",
    "        features['next'] = (' '.join(next_words))\n",
    "    else: \n",
    "        features['next'] = ('')\n",
    "\n",
    "      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a0ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca los ingredientes\n",
    "\"\"\"\n",
    "# Se obtienen los vecinos de las palabras conocidas como ingredientes.\n",
    "def get_ngrams_ingredients(tokenized_words, context, ingredients, dist= 5):\n",
    "    ngrams_ingredients = []    \n",
    "    complements = []\n",
    "    tokenized_ingredients = tokenized_words.copy()\n",
    "    \n",
    "    punct = []\n",
    "\n",
    "    for items in tokenized_words:\n",
    "        idx = items[0]\n",
    "        text = items[1]\n",
    "        data_type = items[6]\n",
    "        \n",
    "        for ingredient in ingredients:\n",
    "            complements = []\n",
    "            \n",
    "            if ingredient == text and data_type != \"complement\":\n",
    "                # Referencia de que el ingrediente ha sido encontrado\n",
    "                tokenized_words[idx][6] = \"ingredient\"\n",
    "              \n",
    "                # Se busca al complemento en palabras NEXT, y si lo encuentra, ya no se busca el otro ingrediente.                \n",
    "                next_word_idx  = idx + 1\n",
    "                \n",
    "                next_word_data = tokenized_words[next_word_idx]\n",
    "                next_word_text =  next_word_data[1]\n",
    "                next_word_tag  = next_word_data[3]\n",
    "                \n",
    "                #validaciones para determinar donde agregar \",\"\n",
    "                #determinamos el genero de la palabra\n",
    "                if tokenized_words[idx][8] != tokenized_words[next_word_idx][8] and len(tokenized_words[next_word_idx][8]) > 0:\n",
    "                    punct.append(idx)\n",
    "                \n",
    "                #determinamos si hay 2 ingredientes seguidos\n",
    "                if tokenized_words[idx - 1][6] == \"ingredient\" and tokenized_words[idx][6] == \"ingredient\" :\n",
    "                    punct.append(idx - 1)\n",
    "                 \n",
    "                wsd_words = wsd_caracteristicas_colocacion(context, ingredient, idx, dist)\n",
    "                \n",
    "                # Se analiza si existe una adposición o pronombre después del ingrediente\n",
    "                if next_word_tag == \"ADP\":                    \n",
    "                    # Referencia de que una adposición ha sido encontrada\n",
    "                    tokenized_words[next_word_idx][6] = \"complement\"\n",
    "                    \n",
    "                    complements.append(next_word_text)\n",
    "                    \n",
    "                    # Se buscan pronombres despúes de la adposición\n",
    "                    for idx in range(next_word_idx + 1, len(tokenized_words)):\n",
    "                        found_word = tokenized_words[idx]\n",
    "                        noun_word = found_word[1]\n",
    "                        noun_tag = found_word[3]\n",
    "                        \n",
    "                        if noun_tag == \"NOUN\":\n",
    "                            tokenized_words[next_word_idx + 1][6] = \"complement\"\n",
    "                            complements.append(noun_word)\n",
    "                        elif noun_tag != \"NOUN\":\n",
    "                              break\n",
    "                    \n",
    "                    if len(complements) == 1:\n",
    "                        complements = []\n",
    "                    \n",
    "                ingredient_data = { \n",
    "                    text:\n",
    "                        {\n",
    "                            \"ngrams\": wsd_words,\n",
    "                            \"ing_idx\": idx,\n",
    "                            \"complements\": complements \n",
    "                        }\n",
    "                }\n",
    "                \n",
    "                ngrams_ingredients.append(ingredient_data)\n",
    "\n",
    "    #actualizamos los id de los ingredientes en el ngrams_ingredients para que coincidan los valores\n",
    "    if len(punct) > 0:\n",
    "\n",
    "        for indx, ng_ing in enumerate(ngrams_ingredients):\n",
    "            key, value = next(iter(ng_ing.items()))\n",
    "\n",
    "            if value[\"ing_idx\"] in punct:\n",
    "                ngrams_ingredients[indx][key][\"ing_idx\"] = value[\"ing_idx\"] + punct.index(value[\"ing_idx\"]) + 1\n",
    "\n",
    "                \n",
    "    return  ngrams_ingredients, tokenized_ingredients, punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "148ba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca las cantidades\n",
    "\"\"\"\n",
    "# Se obtienen los vecinos de las palabras con números.\n",
    "def get_ngrams_quantity(tokenized_words, context, dist= 4):\n",
    "    ngrams_numbers = []\n",
    "\n",
    "    for items in tokenized_words:\n",
    "        idx = items[0]\n",
    "        text = items[1]\n",
    "        lemma = items[2]\n",
    "        tag = items[3]\n",
    "    \n",
    "        # Se obtienen solo los elementos que se encuentran después de un número\n",
    "        if tag == \"NUM\" :#or lemma == \"uno\":\n",
    "            number_data = { \n",
    "                text : \n",
    "                    { \n",
    "                        \"ngrams\": wsd_caracteristicas_colocacion(context, text, idx, dist=4),\n",
    "                        \"num_idx\": idx \n",
    "                    } \n",
    "            }\n",
    "            \n",
    "            ngrams_numbers.append(number_data)\n",
    "    \n",
    "    return ngrams_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "834d90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca las medidas\n",
    "\"\"\"\n",
    "# Se obtienen los vecinos de las palabras con medidas.\n",
    "def get_ngrams_measures(tokenized_words, context, measures_set):\n",
    "    ngrams_measures = []\n",
    "\n",
    "    for items in tokenized_words:\n",
    "        idx = items[0]\n",
    "        text = items[1]\n",
    "    \n",
    "        for measure in measures_set:\n",
    "            if measure == text:\n",
    "                measure_data = {\n",
    "                    text: \n",
    "                    {\n",
    "                        \"ngrams\": wsd_caracteristicas_colocacion(context, measure, idx, dist=4),\n",
    "                        \"measure_idx\": idx \n",
    "                    }\n",
    "                }\n",
    "    \n",
    "                ngrams_measures.append(measure_data)\n",
    "\n",
    "    return ngrams_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3524e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los ingredientes con sus medidas correspondientes.\n",
    "\"\"\"\n",
    "def get_ingredients_with_measures(tokenized_words, ngrams_measures, ingredients, wtn):\n",
    "    ingredients_with_measures = {}\n",
    "    \n",
    "    for ngrams in ngrams_measures:\n",
    "        for measure, measure_data in ngrams.items():\n",
    "            ngrams = measure_data['ngrams']\n",
    "            measure_idx = measure_data['measure_idx']\n",
    "        \n",
    "            prev_words = ngrams[\"previous\"]\n",
    "            next_words = ngrams[\"next\"]\n",
    "  \n",
    "            text = prev_words + \" \" + next_words\n",
    "    \n",
    "            integer = wtn.parse(text)\n",
    "           \n",
    "            # Se buscan los ingredients en los vecinos\n",
    "            for idx, word in enumerate(next_words.split()):\n",
    "                found = False \n",
    "                current_idx_word = measure_idx + (idx + 1)\n",
    "            \n",
    "                #  No itera más si hay una \"y\" y despúes de la \"y\" existe un pronombre\n",
    "                if word == \"y\" and tokenized_words[current_idx_word + 1][3] == \"NOUN\":\n",
    "                    break\n",
    "            \n",
    "                for ingredient in ingredients: # TODO merge ingredientes \n",
    "                    if ingredient not in ingredients_with_measures and ingredient == word and integer != 0:\n",
    "                        ingredients_with_measures[ingredient] = [ integer, measure, current_idx_word] \n",
    "                        found = True # no busca en los vecinos despúes de haber encontrado el ingrediente\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "            \n",
    "    return ingredients_with_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f60d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los ingredientes con sus medidas irregulas correspondientes.\n",
    "\"\"\"\n",
    "def get_ingredients_with_irregular_measures(irregular_measures_set, ngrams_ingredients):\n",
    "    ingredients_with_irregular_measures = {}\n",
    "    \n",
    "    measure_list = irregular_measures_set\n",
    "   \n",
    "    for ngrams in ngrams_ingredients:\n",
    "        for ingredient, ingredient_data in ngrams.items():\n",
    "            ngrams = ingredient_data['ngrams']\n",
    "            ing_idx = ingredient_data['ing_idx']\n",
    "        \n",
    "            prev_words = ngrams[\"previous\"].split()\n",
    "            \n",
    "            last_prev_word = prev_words[-1]\n",
    "            \n",
    "            if last_prev_word == \"de\": #TODO optimizar\n",
    "                del prev_words[-1]\n",
    "                last_prev_word = prev_words[-1]\n",
    "        \n",
    "            if last_prev_word in measure_list:\n",
    "                ingredients_with_irregular_measures[ingredient] = [ \"\", last_prev_word, ing_idx] \n",
    "    \n",
    "    return ingredients_with_irregular_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9643b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca los ingredientes con sus cantidades correspondientes,\n",
    "de la lista de n_grams_ingredients.\n",
    "\"\"\"\n",
    "def find_ingredients_with_quantity(ngrams_ingredients, tokenized_words, ingredients_with_measures, wtn):\n",
    "    ingredients_with_quantity = []\n",
    "\n",
    "    # un platano y medio\n",
    "    for ngrams in ngrams_ingredients:\n",
    "        complet_ingredient = []\n",
    "    \n",
    "        for ingredient, ingredient_data in ngrams.items():\n",
    "            ngrams = ingredient_data['ngrams']\n",
    "            ing_idx = ingredient_data['ing_idx']\n",
    "    \n",
    "            prev_words = ngrams[\"previous\"]\n",
    "            next_words = ngrams[\"next\"]\n",
    "   \n",
    "            text = prev_words + \" \" + next_words #TODO \n",
    "            \n",
    "            invalid = False\n",
    "           \n",
    "            if wtn.parse(prev_words) != 0:\n",
    "                spl = prev_words.split()  \n",
    "                for i, word in enumerate(spl):\n",
    "                    idx = ing_idx - i\n",
    "                    if idx == ing_idx:\n",
    "                        continue\n",
    "                    searched_word = tokenized_words[ing_idx - i]\n",
    "                    if searched_word[1] == \"minutos\":\n",
    "                        invalid = True\n",
    "                        break\n",
    "                    \n",
    "                if invalid:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "            integer = wtn.parse(text)\n",
    "        \n",
    "        # Se comprueba si el elemento ya existe en la lista \"ingredients_with_measures\" o si no contiene una cantidad\n",
    "            existing_idx = [im[2] for im in ingredients_with_measures.values() if im[2] == ing_idx]\n",
    "        \n",
    "            if len(existing_idx) == 1:\n",
    "                break\n",
    "           \n",
    "            if integer != 0:\n",
    "                \n",
    "                found_item = [ ingredient, integer, \"\", ing_idx]\n",
    "            else:\n",
    "                found_item = [ ingredient, \"\", \"\", ing_idx ]\n",
    "        \n",
    "            if found_item not in ingredients_with_quantity:\n",
    "                ingredients_with_quantity.append(found_item)\n",
    "        \n",
    "    return ingredients_with_quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43670b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encuentra solo ingredientes sin informaciones extras.\n",
    "\"\"\"\n",
    "def find_only_ingredients(ingredients_with_quantity):\n",
    "    only_ingredients = []\n",
    "\n",
    "    for iq in ingredients_with_quantity: \n",
    "        ing_quantity = iq[1]\n",
    "        if ing_quantity == '':\n",
    "            only_ingredients.append(iq)\n",
    "    return only_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5c53b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Une los ingredientes con todas sus informaciones encontradas.\n",
    "\"\"\"\n",
    "def merge_ingredients_measures_quantity(ingredients_with_measures, ingredients_with_irregular_measures, ingredients_with_quantity, only_ingredients):\n",
    "    final_ingredients = {}\n",
    "    \n",
    "    # Agregar ingredientes con medidas y cantidades\n",
    "    for im in ingredients_with_measures.items():\n",
    "        ingredient_m = im[0]\n",
    "        quantity_m = im[1][0]\n",
    "        measure_m = im[1][1]\n",
    "        index_m = im[1][2]\n",
    "    \n",
    "        for iq in ingredients_with_quantity:\n",
    "            ingredient_q = iq[0]\n",
    "            ing_quantity = iq[1]\n",
    "        \n",
    "            # Ingredientes con cantidad y medidas\n",
    "            if ingredient_m == ingredient_q: \n",
    "                final_ingredients[ingredient_m] = [ quantity_m, measure_m, index_m ] #optimizar\n",
    "           \n",
    "    # Agregar ingredientes con medidas irregulares\n",
    "    for iim in ingredients_with_irregular_measures.items():\n",
    "        ingredient_iim = iim[0]\n",
    "     \n",
    "        if ingredient_iim in final_ingredients:\n",
    "            continue\n",
    "             \n",
    "        final_ingredients[ingredient_iim] = iim[1]\n",
    "        \n",
    "        \n",
    "    # Agregar solo ingredientes con cantidades\n",
    "    for iq in ingredients_with_quantity:\n",
    "        ingredient_q = iq[0]\n",
    "        quantity_q = iq[1]\n",
    "    \n",
    "        if ingredient_q in final_ingredients or quantity_q == '':\n",
    "            continue\n",
    "            \n",
    "        iq_copied = iq.copy()\n",
    "        iq_copied.remove(ingredient_q)\n",
    "        \n",
    "        final_ingredients[ingredient_q] = iq_copied\n",
    "    \n",
    "    # Agregar solo ingredientes\n",
    "    for oi in only_ingredients:\n",
    "        only_ing = oi[0]\n",
    "    \n",
    "        if only_ing not in final_ingredients:\n",
    "            oi_copied = oi.copy()\n",
    "            oi_copied.remove(only_ing)\n",
    "            final_ingredients[only_ing] = oi_copied \n",
    "\n",
    "    # Se ordenan por index en el texto\n",
    "    final_ingredients = dict(sorted(final_ingredients.items(), key=lambda item: item[1][2]))\n",
    "    return final_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "766e247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene adjetivos de los ingredientes\n",
    "\"\"\" \n",
    "def get_adjectives(tokenized_words, final_ingredients):\n",
    "    final_ingredients_adj = {}\n",
    "\n",
    "    for v_ingredient in final_ingredients.values():\n",
    "        adj = []\n",
    "        v_ingredient_idx = v_ingredient[2]\n",
    "    \n",
    "        ingredient = tokenized_words[v_ingredient_idx][1]\n",
    "    \n",
    "        for t_word in tokenized_words:\n",
    "            t_word_idx = t_word[0]\n",
    "            t_word_word = t_word[1]\n",
    "            t_word_tag = t_word[3]\n",
    "            t_word_det = t_word[4]\n",
    "    \n",
    "            if t_word_idx == v_ingredient_idx:\n",
    "                for idx in range(v_ingredient_idx + 1, len(tokenized_words)): # cambiar a infinito\n",
    "                    searched_adj = tokenized_words[idx]\n",
    "                    adj_word = searched_adj[1]\n",
    "                    adj_tag = searched_adj[3]\n",
    "                    adj_dep = searched_adj[5]\n",
    "                       \n",
    "                    if adj_dep == \"amod\" or adj_dep == \"nmod\" :\n",
    "                        adj.append(adj_word)\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "        final_ingredients_adj[ingredient] = adj\n",
    "    \n",
    "    return final_ingredients_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32238000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene complementos de los ingredientes\n",
    "\"\"\" \n",
    "def get_complements(ngrams_ingredients, final_ingredients, final_ingredients_adj):\n",
    "    ingredients_complement = {}\n",
    "\n",
    "    for v_ingredient in final_ingredients:\n",
    "        complements_data = []\n",
    "    \n",
    "        v_ingredient_idx = final_ingredients[v_ingredient][2]\n",
    "        \n",
    "        for ngrams in ngrams_ingredients:\n",
    "            for ingredient, ingredient_data in ngrams.items():\n",
    "                complements = ingredient_data['complements']\n",
    "                ing_idx = ingredient_data['ing_idx']\n",
    "            \n",
    "                if v_ingredient_idx == ing_idx and len(complements) > 0:\n",
    "                    ingredients_complement[ingredient] = complements\n",
    "                \n",
    "        final_ingredients_adj.update(ingredients_complement)\n",
    "        \n",
    "    return final_ingredients_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa91f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene los ingredientes con toda su información correspondiente como diccionario.\n",
    "\"\"\"\n",
    "def merge_ingredients_data(final_ingredients, final_ingredients_adj):\n",
    "    total_ingredients = {}\n",
    "    \n",
    "    for index, ingredient in enumerate(final_ingredients):\n",
    "        q_data_original = final_ingredients[ingredient].copy()\n",
    "        q_data = q_data_original[:-1]\n",
    "    \n",
    "        if ingredient not in final_ingredients_adj:\n",
    "            total_ingredients[ingredient] = { \"quantity\": q_data , \"description\": [] }\n",
    "            continue\n",
    "        \n",
    "        adj_data = final_ingredients_adj[ingredient]\n",
    "    \n",
    "        total_ingredients[ingredient] = { \"quantity\": q_data , \"description\": adj_data }\n",
    "    \n",
    "    return total_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "943cbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredients():\n",
    "    # Se obtienen los ingredientes y las tres primeras columnas.\n",
    "    ingredients_set = get_ingredients_from_file('ingredientes.xlsx')\n",
    "\n",
    "    main_ingredient = ingredients_set.iloc[:, 0].str.lower()\n",
    "    complementary1  = ingredients_set.iloc[:, 1].str.lower()\n",
    "    complementary2  = ingredients_set.iloc[:, 2].str.lower()\n",
    "    \n",
    "    merge_ingredients_with_description(main_ingredient, complementary1, complementary2)\n",
    "    \n",
    "    # Se obtienen los datos de los datasets\n",
    "    return get_ingredients(main_ingredient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f3fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ingredient(token):\n",
    "    ingr = ingredients()\n",
    "    return token.text.lower() in ingr is not False\n",
    "\n",
    "Token.set_extension('is_ingredient', getter=is_ingredient, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e26fe477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convierte los datos de los ingredientes a texto.\n",
    "\"\"\"\n",
    "def ingredients_data_to_string(data_ingredients):\n",
    "    ingredients_to_text = \"\"\n",
    "    \n",
    "    for t_ingre in data_ingredients:\n",
    "        cantidad = data_ingredients[t_ingre]['quantity'][0]\n",
    "        medida = data_ingredients[t_ingre]['quantity'][1]\n",
    "        descripcion = ' '.join(data_ingredients[t_ingre]['description'])\n",
    "   \n",
    "        if cantidad and medida:\n",
    "            medida += \" de\" \n",
    "        \n",
    "        ingredient_array = [str(cantidad), medida, t_ingre, descripcion]\n",
    "        ingredient_string = \" \".join(ingredient_array)\n",
    "        ingredient_string = re.sub(' +',' ', ingredient_string)\n",
    "        \n",
    "        ingredients_to_text += \"\\n\" + ingredient_string\n",
    "        \n",
    "    return ingredients_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d470e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se inserta una \",\" entre las palabras del texto que corresponda.\n",
    "\"\"\"\n",
    "def insert_punct(tokenized_words, position):\n",
    "\n",
    "    # invertimos la lista de posiciones para agregar la \",\" de atras hacia adelante pra no alterar los ids de posiciones recibidos y ubicarlo en el lugar correcto\n",
    "    reverse_position = sorted(position, reverse=True)\n",
    "\n",
    "    #recorremos las posiciones y las buscamos en el texto, le sumamos una posicion y agregamos la \",\" y actualizamos el valor del tetxo de  que requiere punt en True\n",
    "    for pos in reverse_position:\n",
    "        tokenized_words[pos][7] = True\n",
    "        tokenized_words.insert(pos + 1, [pos + 1, ',', ',', 'PUNCT', 'punctuation', 'punct', '', False, [], []])\n",
    "\n",
    "    #actualizamos las properties correspondientes a cada palabra\n",
    "    text = [ token[1] for token in tokenized_words]\n",
    "    doc = nlp(\" \".join(text))\n",
    "    parsed_text = parse_text(doc, tokenized_words)\n",
    "\n",
    "    return get_tokenized_words(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ed0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los ingredientes de un texto.\n",
    "\"\"\"\n",
    "def get_ingredients_from_text(parsed_text):\n",
    "    tokenized_words = get_tokenized_words(parsed_text)\n",
    "\n",
    "    words_text = get_word_text(tokenized_words)\n",
    "    \n",
    "    # Se obtienen los datos de los datasets\n",
    "    list_ingredients = ingredients()\n",
    "    measures_set = get_measures_from_file('medidas.json')\n",
    "    \n",
    "    # Medidas que nunca tienen cantidades\n",
    "    irregular_measures_set = parse_json_file('medidas irregulares.json')\n",
    "    \n",
    "    # Se inicializa la clase\n",
    "    wtn = WordsToInt()\n",
    "    \n",
    "    # Se obtienen los vecinos de los datos necesarios (ingredientes, cantidades y medidas)\n",
    "    ngrams_ingredients, tokenized_ingredients, position_punct = get_ngrams_ingredients(tokenized_words, words_text, list_ingredients)\n",
    "\n",
    "    tokenized_words = insert_punct(tokenized_words, position_punct)\n",
    "        \n",
    "    ngrams_numbers = get_ngrams_quantity(tokenized_words, words_text)\n",
    "    ngrams_measures = get_ngrams_measures(tokenized_words, words_text, measures_set)\n",
    "    \n",
    "    # Se obtienen los datos de los ingredientes encontrados\n",
    "    ingredients_with_measures = get_ingredients_with_measures(tokenized_words, ngrams_measures, list_ingredients, wtn)\n",
    "    ingredients_with_irregular_measures = get_ingredients_with_irregular_measures(irregular_measures_set, ngrams_ingredients)\n",
    "    ingredients_with_quantity = find_ingredients_with_quantity(ngrams_ingredients, tokenized_words, ingredients_with_measures, wtn)\n",
    "\n",
    "    only_ingredients = find_only_ingredients(ingredients_with_quantity)\n",
    "     \n",
    "    # Se obtienen todos los ingredientes encontrados\n",
    "    final_ingredients = merge_ingredients_measures_quantity(ingredients_with_measures, ingredients_with_irregular_measures, ingredients_with_quantity, only_ingredients)\n",
    "   \n",
    "    # Se obtienen los adjetivos y complementos\n",
    "    final_ingredients_adj = get_adjectives(tokenized_words, final_ingredients)\n",
    "    final_ingredients_adj = get_complements(ngrams_ingredients, final_ingredients, final_ingredients_adj)\n",
    "    \n",
    "    # Se unen las informaciones de los ingredientes encontrados\n",
    "    return merge_ingredients_data(final_ingredients, final_ingredients_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "147045a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mide el grado de similitud entre dos palabras.\n",
    "\"\"\"\n",
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd6d964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se realiza un pre-procesamiento de ingredientes, para eliminar los elementos repetidos o parecidos.\n",
    "\"\"\" \n",
    "def preprocess_ingredients(parsed_ingredients):  \n",
    "    ingredients_text = \" \".join(parsed_ingredients)\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    copy_parsed_ingredients = parsed_ingredients.copy()\n",
    "\n",
    "    # Se verificar los elementos similares.\n",
    "    # El elemento con índice mayor será eliminado\n",
    "    # si tiene similaridad de 80%.\n",
    "    for i, initial_ingred in enumerate(parsed_ingredients):\n",
    "        for io, other_ingred in enumerate(parsed_ingredients):\n",
    "            if i >= io:\n",
    "                continue\n",
    "    \n",
    "            similarity = jaccard_similarity(initial_ingred, other_ingred)\n",
    "            if similarity > 0.80:\n",
    "                del copy_parsed_ingredients[other_ingred]\n",
    "    \n",
    "    return ingredients_data_to_string(copy_parsed_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c4572db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients():\n",
    "    textfile = os.getcwd() + \"\\\\Converted results\\\\\" + \"Converted_audio.txt\"\n",
    "    last_ingredients = \"\"\n",
    "\n",
    "    try:\n",
    "        with open(textfile) as file:\n",
    "            fileContent = file.read()\n",
    "            file.close()\n",
    "    except FileNotFoundError:\n",
    "         print(f'The file {textfile} does not exist')\n",
    "    else:\n",
    "        parsed_text = parse_text(fileContent)\n",
    "        from_description = get_text_from_description(parsed_text)\n",
    "    \n",
    "        if len(from_description) == 0:\n",
    "            parsed_ingredients = get_ingredients_from_text(parsed_text)\n",
    "            last_ingredients = preprocess_ingredients(parsed_ingredients)\n",
    "        else:\n",
    "            last_ingredients = \" \".join(from_description)\n",
    "    \n",
    "\n",
    "    # Crea el archivo de ingredientes\n",
    "    with open('recipe ingredients.txt', 'w') as f:\n",
    "        f.write(\"INGREDIENTES:\\n\")\n",
    "        f.write(last_ingredients)\n",
    "\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
