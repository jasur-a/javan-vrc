{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83907788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xlrd in /Users/jasura/Library/Python/3.9/lib/python/site-packages (2.0.1)\n",
      "Requirement already satisfied: nltk in /Users/jasura/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in /Users/jasura/Library/Python/3.9/lib/python/site-packages (3.5.1)\n",
      "Requirement already satisfied: pandas in /Users/jasura/Library/Python/3.9/lib/python/site-packages (1.5.3)\n",
      "Requirement already satisfied: sklearn in /Users/jasura/Library/Python/3.9/lib/python/site-packages (0.0.post1)\n",
      "Requirement already satisfied: joblib in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: setuptools in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (67.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip  install xlrd nltk spacy pandas sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6656660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /Users/jasura/Library/Python/3.9/lib/python/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371ca978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef510a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pattern in /Users/jasura/Library/Python/3.9/lib/python/site-packages (3.6)\n",
      "Requirement already satisfied: cherrypy in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (18.8.0)\n",
      "Requirement already satisfied: backports.csv in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (1.0.7)\n",
      "Requirement already satisfied: lxml in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (4.9.2)\n",
      "Requirement already satisfied: pdfminer.six in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (20221105)\n",
      "Requirement already satisfied: future in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pattern) (0.18.2)\n",
      "Requirement already satisfied: numpy in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (1.24.2)\n",
      "Requirement already satisfied: scipy in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (1.10.1)\n",
      "Requirement already satisfied: python-docx in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (0.8.11)\n",
      "Requirement already satisfied: feedparser in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (6.0.10)\n",
      "Requirement already satisfied: nltk in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (3.8.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (4.12.2)\n",
      "Requirement already satisfied: mysqlclient in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (2.1.1)\n",
      "Requirement already satisfied: requests in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pattern) (2.28.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->pattern) (2.4)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cherrypy->pattern) (9.0.0)\n",
      "Requirement already satisfied: portend>=2.1.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cherrypy->pattern) (3.1.0)\n",
      "Requirement already satisfied: more-itertools in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cherrypy->pattern) (9.1.0)\n",
      "Requirement already satisfied: zc.lockfile in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cherrypy->pattern) (3.0.post1)\n",
      "Requirement already satisfied: jaraco.collections in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cherrypy->pattern) (4.1.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from feedparser->pattern) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk->pattern) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk->pattern) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk->pattern) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk->pattern) (4.65.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pdfminer.six->pattern) (39.0.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pdfminer.six->pattern) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests->pattern) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests->pattern) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from requests->pattern) (3.4)\n",
      "Requirement already satisfied: jaraco.functools in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.6.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
      "Requirement already satisfied: tempora>=1.8 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from portend>=2.1.1->cherrypy->pattern) (5.2.2)\n",
      "Requirement already satisfied: jaraco.text in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from jaraco.collections->cherrypy->pattern) (3.11.1)\n",
      "Requirement already satisfied: setuptools in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from zc.lockfile->cherrypy->pattern) (67.6.1)\n",
      "Requirement already satisfied: pycparser in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
      "Requirement already satisfied: pytz in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.7.1)\n",
      "Requirement already satisfied: jaraco.context>=4.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (4.3.0)\n",
      "Requirement already satisfied: autocommand in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.2.2)\n",
      "Requirement already satisfied: inflect in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (6.0.4)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (1.10.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install   pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46aa0c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/jasura/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jasura/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.3.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13a23d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e8f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0015b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nltk.download()\n",
    "#!python -m spacy download es_dep_news_trf\n",
    "#conda install -c conda-forge spacy-model-es_dep_news_trf\n",
    "#pip install xlrd\n",
    "\n",
    "# Librerías para PNL\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import es_dep_news_trf # https://spacy.io/usage/models\n",
    "\n",
    "\n",
    "# Otras librerías\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pattern.es import pluralize\n",
    "from pattern.es import singularize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6939c42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/site-packages/spacy/errors.py\", line 2, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/site-packages/spacy/compat.py\", line 3, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/site-packages/thinc/config.py\", line 2, in <module>\n",
      "    import confection\n",
      "  File \"/Users/jasura/opt/anaconda3/lib/python3.9/site-packages/confection/__init__.py\", line 10, in <module>\n",
      "    from pydantic import BaseModel, create_model, ValidationError, Extra\n",
      "  File \"pydantic/__init__.py\", line 2, in init pydantic.__init__\n",
      "  File \"pydantic/dataclasses.py\", line 39, in init pydantic.dataclasses\n",
      "    # + Value   | Meaning                                 |\n",
      "ImportError: cannot import name dataclass_transform\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "f64fc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "Token.set_extension('need_punct', default=False, force=True)\n",
    "\n",
    "def parse_text(doc, prev_doc = []):    \n",
    "     parsed_text = []\n",
    "\n",
    "     for idx, token in enumerate(doc):\n",
    "          description = prev_doc[idx][6] if len(prev_doc) > 0 else \"\"\n",
    "          punt = prev_doc[idx][7] if len(prev_doc) > 0 else token._.need_punct\n",
    "          parsed_text.append([token.text.lower(), token.lemma_, token.pos_, spacy.explain(token.pos_), \n",
    "                              token.dep_, description, punt, token.morph.get(\"Gender\"), token.morph.get('Number')])\n",
    "\n",
    "     return parsed_text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a60d0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_words(parsed_text):\n",
    "    tokenized_words = []\n",
    "    \n",
    "    stopwords = [\".\", \"mi\", \"hola\", \"mmm\", \"gracias\", \"\\n\", \" \"]\n",
    "\n",
    "    no_stopwords = [word for word in parsed_text if word[2] != \"SPACE\" and word[0] not in stopwords]\n",
    "\n",
    "    for index, word in enumerate(no_stopwords):\n",
    "        word.insert(0, index)\n",
    "        tokenized_words.append(word)\n",
    "        \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbbfbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene solo el texto de las palabras\n",
    "\"\"\"\n",
    "def get_word_text(words):\n",
    "    return [word[1] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363483f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO cuando hay ingredientes en la descripcion se obtiene solo este texto\n",
    "def get_text_from_description(parse_text):\n",
    "    from_description = []\n",
    "\n",
    "    for i, word in enumerate(parse_text):\n",
    "        if word[0] != \"descripción\" or parse_text[i + 1][0] != \"del\" or parse_text[i + 2][0] != \"video\":\n",
    "            continue\n",
    "        for w in range(i + 2, len(parse_text)):\n",
    "            if parse_text[w][0] == \"ingredientes\":\n",
    "                for j in range(w, len(parse_text)):\n",
    "                    from_description.append(parse_text[j][0])\n",
    "                    \n",
    "    if len(from_description) == 0:\n",
    "        return []\n",
    "    else:  \n",
    "        return from_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6699976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define la palabra y la convierte a plural usando la función pluralize().\n",
    "\"\"\" \n",
    "def to_plural(word):\n",
    "    # #https://stackoverflow.com/questions/31387905/converting-plural-to-singular-in-a-text-file-with-python\n",
    "    # Installing NLTK data to import\n",
    "    # and run en module of pattern\n",
    "    #nltk.download('popular')\n",
    "\n",
    "    return pluralize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8cae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define la palabra y la convierte a singular usando la función singularize().\n",
    "\"\"\" \n",
    "def to_singular(word):\n",
    "    # # Import the NLTK module https://www.geeksforgeeks.org/python-program-to-convert-singular-to-plural/\n",
    "    return singularize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f488420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene los ingredientes del dataset.\n",
    "\"\"\"\n",
    "def get_ingredients_from_file(dataset):\n",
    "    return pd.read_excel(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8befb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une los ingredientes con sus respectivos complementos descriptivos. #TODO eliminar\n",
    "def merge_ingredients_with_description(main_ingredient, complementary1, complementary2):\n",
    "    data = []\n",
    "    data = main_ingredient + (' ' + complementary1).fillna('') + (' ' + complementary2).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67c66940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocesar información (se convierte la primera columna a minusculas y se eliminan NAN)\n",
    "\"\"\" \n",
    "def get_ingredients(main_ingredient):\n",
    "    basic_ingredients = main_ingredient.dropna().T.drop_duplicates().T\n",
    "\n",
    "    ingredients = []\n",
    "    for ing in basic_ingredients:\n",
    "        ingredients.append(to_plural(ing))\n",
    "        ingredients.append(to_singular(ing))\n",
    "        \n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43d99eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los datos de un archivo json.\n",
    "\"\"\"\n",
    "def parse_json_file(jsonfile):\n",
    "    with open(jsonfile) as user_file:\n",
    "        measures_content = user_file.read()\n",
    "        parsed_json = json.loads(measures_content)\n",
    "    return parsed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1efbb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene las medidas del dataset.\n",
    "\"\"\" \n",
    "def get_measures_from_file(jsonfile):\n",
    "    measures_set = []\n",
    "    \n",
    "    parsed_json = parse_json_file(jsonfile)\n",
    "    \n",
    "    for liq_items in parsed_json['liquidos']:\n",
    "        measures_set.append(liq_items)\n",
    "\n",
    "    for sol_items in parsed_json['solido']:\n",
    "        measures_set.append(sol_items)\n",
    "    \n",
    "    return measures_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b2ead00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clase que puede traducir cadenas de palabras numéricas \n",
    "comunes en espanol y convertirlas en la cantidad numérica correspondiente.\n",
    "\"\"\"\n",
    "class WordsToInt():\n",
    "    # Asignación de dígitos a nombres relativos a unidades \"ones\".\n",
    "    __ones__ = { 'un':   1, 'una':   1, 'uno':   1, 'once':     11,\n",
    "                 'dos':   2, 'doce':     12,\n",
    "                 'tres': 3, 'trece':   13,\n",
    "                 'cuatro':  4, 'catorce':   14,\n",
    "                 'cinco':  5, 'quince':    15,\n",
    "                 'seis':   6, 'dieciseis':    16,\n",
    "                 'siete': 7, 'diecisiete':  17,\n",
    "                 'ocho': 8, 'dieciocho':   18,\n",
    "                 'nueve':  9, 'diecinueve':   19,\n",
    "                 'diez': 10\n",
    "               }\n",
    "    \n",
    "    # Asignación de dígitos a nombres relativos 'decenas'.\n",
    "    __tens__ = { \n",
    "                 'veinte':  20,\n",
    "                 'treinta':  30,\n",
    "                 'cuarenta':   40,\n",
    "                 'cincuenta':   50,\n",
    "                 'sesenta':   60,\n",
    "                 'setenta': 70,\n",
    "                 'ochenta':  80,\n",
    "                 'noventa':  90 \n",
    "    }\n",
    "    \n",
    "    # Asignación de dígitos a nombres relativos 'centenas'.\n",
    "    __hundreds__ = {\n",
    "                'cien': 100, 'ciento': 100,\n",
    "                'quinientos': 500,\n",
    "                'setecientos': 700,\n",
    "                'mil':  1000,\n",
    "    }\n",
    "    \n",
    "    # Lista ordenada de los nombres asignados a los grupos de miles.\n",
    "    __groups__ = { \n",
    "                   'mil':  1000,\n",
    "                   'millon':   1000000,\n",
    "                   'billoon':   1000000000,\n",
    "                   'trillon':  1000000000000 \n",
    "    }\n",
    "    \n",
    "    # Asignación de dígitos a nombres relativos de 'fracciones'.\n",
    "    __fraction__ = { 'medio':  2, 'media': 2, 'tercio': 5, 'cuarto': 4, \"cuarta\": 5 }\n",
    "\n",
    "    # Expresión regular que busca nombres de grupos de números y captura:\n",
    "    # 1- la cadena que precede al nombre del grupo, y\n",
    "    # 2- el nombre del grupo (o una cadena vacía si el\n",
    "    # el valor capturado es simplemente el final de la cadena\n",
    "    # indicando el grupo 'unos', que normalmente es\n",
    "    # no expresado)\n",
    "    __groups_re__ = re.compile(\n",
    "        r'\\s?([\\w\\s]+?)(?:\\s((?:%s))|$)' %\n",
    "        ('|'.join(__groups__))\n",
    "        )\n",
    "\n",
    "    # Expresión regular que busca dentro de un solo grupo de números para\n",
    "    # 'n cien' y captura:\n",
    "    # 1- la cadena que precede de 'cientos', y\n",
    "    # 2- la cadena que sigue al 'cientos'.\n",
    "    # Se considerará como el número que indica el\n",
    "    # valor posicional de las decenas y las unidades del grupo.\n",
    "    __hundreds_re__ = re.compile(r'([\\w\\s]+)cientos(?:\\s(.*)|$)')\n",
    "    \n",
    "    __hundreds_extra__ = re.compile(r'([\\w]+)(?:\\s(.*)|$)')\n",
    "\n",
    "    # Expresión regular que se ve dentro de un solo número o\n",
    "    # grupo al que ya se le extrajo su valor de 'cientos'\n",
    "    # para un patrón de 'decenas' (es decir, 'cuarenta y dos') y captura:\n",
    "    #1- las decenas\n",
    "    #2- los unos\n",
    "    __tens_and_ones_re__ =  re.compile(\n",
    "        r'((?:%s))(?:\\s(.*)|$)' %\n",
    "        ('|'.join(__tens__.keys()))\n",
    "        )\n",
    "    \n",
    "    # Expresión regular que busca dentro de un solo grupo de números para\n",
    "    # 'fracción' y captura:\n",
    "    # 1- el primer número ordinario (1)\n",
    "    # 2- el nombre fraccionario (4)\n",
    "    # Al final da 1/4\n",
    "    __fraction_re__ = re.compile(\n",
    "        r'\\s?([\\w\\s]+?)(?:\\s((?:%s))|$)' %\n",
    "        ('|'.join(__fraction__))\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    Analiza las palabras hasta encontrar número que describen.\n",
    "    \"\"\"\n",
    "    def parse(self, words):\n",
    "        # Se van a analizar las palabras con ciertos tags o lemas.\n",
    "        #nlp = spacy.load(\"es_dep_news_trf\")\n",
    "        \n",
    "        words = words.lower().strip()\n",
    "       \n",
    "        # Crea una lista para guardar los grupos de números tal como los encontramos dentro\n",
    "        # la cadena de palabras.\n",
    "        groups = {}        \n",
    "        # Crea una variable para guardar un número.\n",
    "        num = 0\n",
    "         \n",
    "        # Se dividen las palabras para saber si existe algun número en las listas numéricas.\n",
    "        split_words = words.split()\n",
    "        updated_words = \"\"\n",
    "        \n",
    "        indexes = []\n",
    "        found_words = []\n",
    "        firstNumber = []\n",
    "        \n",
    "        for idx, word in enumerate(split_words):\n",
    "            # Se tokeniza la palabra numérica.\n",
    "            doc = nlp(word)\n",
    "            \n",
    "            tokenized_number = [[ w.pos_, w.lemma_, w.text] for w in doc][0]\n",
    "            \n",
    "            # Número no puede ser igual al anterior\n",
    "            if idx > 0 and word == split_words[idx - 1]:\n",
    "                continue\n",
    "            \n",
    "            if tokenized_number[0] == \"NUM\" or (tokenized_number[2] == \"uno\" or tokenized_number[2] == \"un\" or tokenized_number[2] != \"unas\" and tokenized_number[2] == \"una\") or tokenized_number[1] == \"cuarto\":\n",
    "          #  if tokenized_number[0] == \"NUM\" or tokenized_number[2] == \"cuarto\" or (tokenized_number[2] == \"uno\" or tokenized_number[2] == \"un\" and tokenized_number[2] == \"una\"):\n",
    "                updated_words += \" \" + word\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Se crea nuevamente una cadena de texto más limpio.\n",
    "        updated_words = updated_words.strip()\n",
    "        \n",
    "        # En caso de que una palabra contenga una fracción.\n",
    "        has_fraction = [True for fr_word in split_words if fr_word in WordsToInt.__fraction__.keys()]\n",
    "        \n",
    "        if len(split_words) > 1 and len(has_fraction) > 0 and has_fraction[0]:\n",
    "            group_fraction = \"\"\n",
    "            for group in WordsToInt.__fraction_re__.findall(updated_words):\n",
    "                fraction_match = WordsToInt.__fraction_re__.match(group[0])\n",
    "            \n",
    "                if group[1] in WordsToInt.__fraction__:\n",
    "                    tens_and_ones = group[0]\n",
    "                    \n",
    "                    if group[0] in WordsToInt.__fraction__:\n",
    "                        return \"1/\" + str(WordsToInt.__fraction__[group[0]])\n",
    "                    \n",
    "                    if tens_and_ones not in WordsToInt.__ones__.keys():\n",
    "                        split_ones = tens_and_ones.split()\n",
    "                        tens_and_ones = split_ones[0]\n",
    "                \n",
    "                    if tens_and_ones in WordsToInt.__ones__.keys():\n",
    "                        group_fraction = str(WordsToInt.__ones__[tens_and_ones]) + \"/\" + str(WordsToInt.__fraction__[group[1]])\n",
    "                \n",
    "            return group_fraction\n",
    "            \n",
    "        # Para números enteros\n",
    "        for group in WordsToInt.__groups_re__.findall(updated_words):\n",
    "            # Determina la posición de este grupo de números\n",
    "            # dentro del número entero.\n",
    "            # Se asume que el índice de grupo es el grupo first/ones\n",
    "            # hasta que se determine que es un grupo superior.\n",
    "            group_multiplier = 1\n",
    "            group_num = 0\n",
    "            \n",
    "             # Determina el valor de este grupo de números\n",
    "            if group[1] in WordsToInt.__groups__:\n",
    "                group_multiplier = WordsToInt.__groups__[group[1]]\n",
    "           \n",
    "            # Crea una variable para guardar lo que queda cuando\n",
    "            # se eliminan las \"centenas\" (es decir, los valores de las decenas y las unidades)\n",
    "            hundreds_match = WordsToInt.__hundreds_re__.match(group[0])\n",
    "            \n",
    "            tens_and_ones = None\n",
    "            \n",
    "            # Se verifica si existe algun digito que coincida con un patrón de \"hundreds extra\".\n",
    "            hundreds_match1 = WordsToInt.__hundreds_extra__.match(group[0])\n",
    "            match_hundreds_extra = hundreds_match1.group(1) in WordsToInt.__hundreds__.keys()\n",
    "            \n",
    "            # En caso de que así sea:\n",
    "            if hundreds_match1 is not None and hundreds_match1.group(1) is not None and match_hundreds_extra:                           \n",
    "                # Se toma el valor del dígito\n",
    "                group_num = WordsToInt.__hundreds__[hundreds_match1.group(1)]\n",
    "                # Se guarda el valor posicional de las decenas y las unidades.\n",
    "                tens_and_ones = hundreds_match1.group(2)\n",
    "                       \n",
    "            # Si hay una cadena en este grupo que coincida con el patrón 'n cien'\n",
    "            elif hundreds_match is not None and hundreds_match.group(1) is not None:\n",
    "                # Multiplica el valor 'n' por 100 e incrementa el valor de este grupo\n",
    "                group_num = group_num + \\\n",
    "                            (WordsToInt.__ones__[hundreds_match.group(1)] * 100)\n",
    "                # Se guarda el valor posicional de las decenas y las unidades.\n",
    "                tens_and_ones = hundreds_match.group(2)\n",
    "            else:\n",
    "            # Si no hubiera ninguna cadena que coincidiera con el patrón 'n cien',\n",
    "            # supone que toda la cadena contiene solo decenas y unidades\n",
    "            # como valores posicionales.\n",
    "                tens_and_ones = group[0]\n",
    "            # Si la cadena de 'decenas y unidades' está vacía, se pasa al siguiente grupo\n",
    "            if tens_and_ones is None:\n",
    "                # Incrementa el número total con el número de grupo actual * su multiplicador\n",
    "                num = num + (group_num * group_multiplier)\n",
    "                continue\n",
    "           # Busca las decenas y las unidades \n",
    "            tn1_match = WordsToInt.__tens_and_ones_re__.match(tens_and_ones)\n",
    "            # Si el patrón coincide, hay un valor posicional de 'decenas'\n",
    "            if tn1_match is not None:\n",
    "                # Agrega las decenas\n",
    "                group_num = group_num + WordsToInt.__tens__[tn1_match.group(1)]\n",
    "                 # Agrega las unidades\n",
    "                if tn1_match.group(2) is not None:\n",
    "                    group_num = group_num + WordsToInt.__ones__[tn1_match.group(2)] \n",
    "            else:\n",
    "            # Asume que las 'decenas y unidades' en realidad solo contenían las unidade.\n",
    "                if tens_and_ones not in WordsToInt.__ones__.keys():\n",
    "                    split_ones = tens_and_ones.split()\n",
    "                    tens_and_ones = split_ones[0]\n",
    "            \n",
    "            if tens_and_ones in WordsToInt.__ones__.keys():\n",
    "                group_num = group_num + WordsToInt.__ones__[tens_and_ones]\n",
    "                \n",
    "            # Incrementa el número total con el número de grupo actual * su multiplicador\n",
    "            num = num + (group_num * group_multiplier)\n",
    "        return num   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "7f6c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los vecinos anteriores y posteriores de una palabra,\n",
    "de acuerdo a una distancia dada.\n",
    "\"\"\"\n",
    "def wsd_caracteristicas_colocacion(context, instance, pos, dist=2, punt = []):\n",
    "    features = {}\n",
    "    con = context\n",
    "    # la coma esta antes\n",
    "    punt_prev = True if pos - 1 in punt else None \n",
    "    #la coma esta despues\n",
    "    punt_next = True if pos in punt else None\n",
    "\n",
    "    # Las palabras serán almacenadas de acuerdo a su posición.\n",
    "    prev_words = []\n",
    "    next_words = []\n",
    "\n",
    "\n",
    "    # A partir de una posición dada, se obtienen las palabras previas requeridas\n",
    "    # dependiendo la distancia.\n",
    "    if not punt_prev:\n",
    "        for i in range(max(0, pos-dist), pos):\n",
    "            prev_words.append(con[i])\n",
    "        features[\"previous\"] = (' '.join(prev_words))\n",
    "    else:\n",
    "        features[\"previous\"] = ('')\n",
    "\n",
    "     # A partir de una posición dada, se obtienen las palabras posteriores requeridas\n",
    "    # dependiendo la distancia.\n",
    "    if not punt_next:\n",
    "        for i in range(pos+1, min(pos+dist+1, len(con))):\n",
    "            next_words.append(con[i])\n",
    "        features['next'] = (' '.join(next_words))\n",
    "    else: \n",
    "        features['next'] = ('')\n",
    "\n",
    "      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "8a0ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca los ingredientes\n",
    "\"\"\"\n",
    "# Se obtienen los vecinos de las palabras conocidas como ingredientes.\n",
    "def get_ngrams_ingredients(tokenized_words, context, ingredients, dist= 5):\n",
    "    ngrams_ingredients = []    \n",
    "    complements = []\n",
    "    tokenized_ingredients = tokenized_words.copy()\n",
    "\n",
    "    punct = []\n",
    "\n",
    "    for items in tokenized_words:\n",
    "        idx = items[0]\n",
    "        text = items[1]\n",
    "        data_type = items[6]\n",
    "        \n",
    "        for ingredient in ingredients:\n",
    "            complements = []\n",
    "            \n",
    "            if ingredient == text and data_type != \"complement\":\n",
    "                \n",
    "                # Referencia de que el ingrediente ha sido encontrado\n",
    "                tokenized_words[idx][6] = \"ingredient\"\n",
    "              \n",
    "                # Se busca al complemento en palabras NEXT, y si lo encuentra, ya no se busca el otro ingrediente.                \n",
    "                next_word_idx  = idx + 1\n",
    "                \n",
    "                next_word_data = tokenized_words[next_word_idx]\n",
    "                next_word_text =  next_word_data[1]\n",
    "                next_word_tag  = next_word_data[3]\n",
    "\n",
    "                #validaciones para determinar donde agregar \",\"\n",
    "                #determinamos el genero de la palabra\n",
    "                if tokenized_words[idx][8] != tokenized_words[next_word_idx][8] and len(tokenized_words[next_word_idx][8]) > 0:\n",
    "                    punct.append(idx)\n",
    "                \n",
    "                #determinamos si hay 2 ingredientes seguidos\n",
    "                if tokenized_words[idx - 1][6] == \"ingredient\" and tokenized_words[idx][6] == \"ingredient\" :\n",
    "                    punct.append(idx - 1)\n",
    "                 \n",
    "\n",
    "                wsd_words = wsd_caracteristicas_colocacion(context, ingredient, idx, dist, punct)\n",
    "\n",
    "\n",
    "                #{'tomates': {'ngrams': {'previous': 'poblanos unos traguitos de queso', 'next': 'una torta de cebolla y'}, 'ing_idx': 41, 'complements': []}}\n",
    "\n",
    "                # Se analiza si existe una adposición o pronombre después del ingrediente\n",
    "                if next_word_tag == \"ADP\": # TODO or tag == \"NOUN\":                    \n",
    "                    # Referencia de que una adposición ha sido encontrada\n",
    "                    tokenized_words[next_word_idx][6] = \"complement\"\n",
    "                    \n",
    "                    complements.append(next_word_text)\n",
    "                    \n",
    "                    # Se buscan pronombres despúes de la adposición\n",
    "                    for idx in range(next_word_idx + 1, len(tokenized_words)):\n",
    "                        found_word = tokenized_words[idx]\n",
    "                        noun_word = found_word[1]\n",
    "                        noun_tag = found_word[3]\n",
    "                        \n",
    "                        if noun_tag == \"NOUN\":\n",
    "                            tokenized_words[next_word_idx + 1][6] = \"complement\"\n",
    "                            complements.append(noun_word)\n",
    "                        elif noun_tag != \"NOUN\":\n",
    "                              #punct.append(next_word_idx)\n",
    "                              break\n",
    "                    \n",
    "                    if len(complements) == 1:\n",
    "                        complements = []\n",
    "                \n",
    "                \n",
    "                ingredient_data = { \n",
    "                    text:\n",
    "                        {\n",
    "                            \"ngrams\": wsd_words,\n",
    "                            \"ing_idx\": idx,\n",
    "                            \"complements\": complements \n",
    "                        }\n",
    "                }\n",
    "                \n",
    "                ngrams_ingredients.append(ingredient_data)\n",
    "\n",
    "    #actualizamos los id de los ingredientes en el ngrams_ingredients para que coincidan los valores\n",
    "    if len(punct) > 0:\n",
    "\n",
    "        for indx, ng_ing in enumerate(ngrams_ingredients):\n",
    "            key, value = next(iter(ng_ing.items()))\n",
    "\n",
    "            if value[\"ing_idx\"] in punct:\n",
    "                ngrams_ingredients[indx][key][\"ing_idx\"] = value[\"ing_idx\"] + punct.index(value[\"ing_idx\"]) + 1\n",
    "\n",
    "    return ngrams_ingredients, tokenized_ingredients, punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "148ba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca las cantidades\n",
    "\"\"\"\n",
    "# Se obtienen los vecinos de las palabras con números.\n",
    "def get_ngrams_quantity(tokenized_words, context, dist= 4):\n",
    "    ngrams_numbers = []\n",
    "\n",
    "    for items in tokenized_words:\n",
    "        idx = items[0]\n",
    "        text = items[1]\n",
    "        lemma = items[2]\n",
    "        tag = items[3]\n",
    "    \n",
    "        # Se obtienen solo los elementos que se encuentran después de un número\n",
    "        if tag == \"NUM\" :#or lemma == \"uno\":\n",
    "            number_data = { \n",
    "                text : \n",
    "                    { \n",
    "                        \"ngrams\": wsd_caracteristicas_colocacion(context, text, idx, dist=4),\n",
    "                        \"num_idx\": idx \n",
    "                    } \n",
    "            }\n",
    "            \n",
    "            ngrams_numbers.append(number_data)\n",
    "    \n",
    "    return ngrams_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "834d90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca las medidas\n",
    "\"\"\"\n",
    "# Se obtienen los vecinos de las palabras con medidas.\n",
    "def get_ngrams_measures(tokenized_words, context, measures_set):\n",
    "    ngrams_measures = []\n",
    "\n",
    "    for items in tokenized_words:\n",
    "        idx = items[0]\n",
    "        text = items[1]\n",
    "    \n",
    "        for measure in measures_set:\n",
    "            if measure == text:\n",
    "                measure_data = {\n",
    "                    text: \n",
    "                    {\n",
    "                        \"ngrams\": wsd_caracteristicas_colocacion(context, measure, idx, dist=4),\n",
    "                        \"measure_idx\": idx \n",
    "                    }\n",
    "                }\n",
    "    \n",
    "                ngrams_measures.append(measure_data)\n",
    "\n",
    "    return ngrams_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "a3524e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los ingredientes con sus medidas correspondientes.\n",
    "\"\"\"\n",
    "def get_ingredients_with_measures(tokenized_words, ngrams_measures, ingredients, wtn):\n",
    "    ingredients_with_measures = {}\n",
    "\n",
    "    for ngrams in ngrams_measures:\n",
    "        for measure, measure_data in ngrams.items():\n",
    "            ngrams = measure_data['ngrams']\n",
    "            measure_idx = measure_data['measure_idx']\n",
    "        \n",
    "            prev_words = ngrams[\"previous\"]\n",
    "            next_words = ngrams[\"next\"]\n",
    "  \n",
    "            text = prev_words + \" \" + next_words #TODO\n",
    "    \n",
    "            integer = wtn.parse(text)\n",
    "           \n",
    "            # Se buscan los ingredients en los vecinos\n",
    "            for idx, word in enumerate(next_words.split()):\n",
    "                found = False \n",
    "                current_idx_word = measure_idx + (idx + 1)\n",
    "            \n",
    "                #  No itera más si hay una \"y\" y despúes de la \"y\" existe un pronombre o un \",\"\n",
    "                if (word == \"y\" and tokenized_words[current_idx_word + 1][3] == \"NOUN\") or word == \",\" :\n",
    "                    break\n",
    "            \n",
    "                for ingredient in ingredients: # TODO merge ingredientes \n",
    "                    if ingredient not in ingredients_with_measures and ingredient == word and integer != 0:\n",
    "                        ingredients_with_measures[ingredient] = [ integer, measure, current_idx_word] \n",
    "                        found = True # no busca en los vecinos despúes de haber encontrado el ingrediente\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "              \n",
    "    return ingredients_with_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "f60d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los ingredientes con sus medidas irregulas correspondientes.\n",
    "\"\"\"\n",
    "def get_ingredients_with_irregular_measures(irregular_measures_set, ngrams_ingredients):\n",
    "    ingredients_with_irregular_measures = {}\n",
    "    \n",
    "    measure_list = irregular_measures_set\n",
    "   \n",
    "    for ngrams in ngrams_ingredients:\n",
    "        for ingredient, ingredient_data in ngrams.items():\n",
    "            ngrams = ingredient_data['ngrams']\n",
    "            ing_idx = ingredient_data['ing_idx']\n",
    "        \n",
    "            if  len(ngrams[\"previous\"]) > 0:\n",
    "\n",
    "                prev_words = ngrams[\"previous\"].split() \n",
    "                \n",
    "                last_prev_word = prev_words[-1]\n",
    "                \n",
    "                if last_prev_word == \"de\": #TODO optimizar\n",
    "                    del prev_words[-1]\n",
    "                    last_prev_word = prev_words[-1]\n",
    "            \n",
    "                if last_prev_word in measure_list:\n",
    "                    ingredients_with_irregular_measures[ingredient] = [ \"\", last_prev_word, ing_idx] \n",
    "    \n",
    "    return ingredients_with_irregular_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "9643b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Busca los ingredientes con sus cantidades correspondientes,\n",
    "de la lista de n_grams_ingredients.\n",
    "\"\"\"\n",
    "def find_ingredients_with_quantity(ngrams_ingredients, tokenized_words, ingredients_with_measures, wtn):\n",
    "    ingredients_with_quantity = []\n",
    "\n",
    "\n",
    "    # un platano y medio\n",
    "    for ngrams in ngrams_ingredients:\n",
    "        complet_ingredient = []\n",
    "    \n",
    "        for ingredient, ingredient_data in ngrams.items():\n",
    "            ngrams = ingredient_data['ngrams']\n",
    "            ing_idx = ingredient_data['ing_idx']\n",
    "    \n",
    "            prev_words = ngrams[\"previous\"]\n",
    "            next_words = ngrams[\"next\"]\n",
    "   \n",
    "            text = prev_words + \" \" + next_words #TODO \n",
    "            number = tokenized_words[ing_idx][9]\n",
    "            invalid = False\n",
    "            # TODO si hay otros pronombres antes o verbos => no contar\n",
    "            if wtn.parse(prev_words) != 0:\n",
    "                spl = prev_words.split()  \n",
    "                for i, word in enumerate(spl):\n",
    "                    idx = ing_idx - i\n",
    "                    if idx == ing_idx:\n",
    "                        continue\n",
    "                    searched_word = tokenized_words[ing_idx - i]\n",
    "                    if searched_word[1] == \"minutos\":\n",
    "                        invalid = True\n",
    "                        break\n",
    "                if invalid:\n",
    "                    break\n",
    "                    \n",
    "            integer = wtn.parse(text)\n",
    "\n",
    "            # Se comprueba si el elemento ya existe en la lista \"ingredients_with_measures\" o si no contiene una cantidad\n",
    "            existing_idx = [im[2] for im in ingredients_with_measures.values() if im[2] == ing_idx]\n",
    "        \n",
    "            if len(existing_idx) == 1:\n",
    "                break\n",
    "\n",
    "            if 'Plur' in number and integer == 1 :\n",
    "\n",
    "                found_item = [ ingredient, \"\", \"\", ing_idx ]\n",
    "           \n",
    "            elif integer != 0:\n",
    "                \n",
    "                found_item = [ ingredient, integer, \"\", ing_idx]\n",
    "            else:\n",
    "            \n",
    "                found_item = [ ingredient, \"\", \"\", ing_idx ]\n",
    "        \n",
    "            if found_item not in ingredients_with_quantity:\n",
    "                ingredients_with_quantity.append(found_item)\n",
    "\n",
    "        \n",
    "    return ingredients_with_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "43670b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encuentra solo ingredientes sin informaciones extras.\n",
    "\"\"\"\n",
    "def find_only_ingredients(ingredients_with_quantity):\n",
    "    only_ingredients = []\n",
    "\n",
    "    for iq in ingredients_with_quantity: \n",
    "        ing_quantity = iq[1]\n",
    "        if ing_quantity == '':\n",
    "            only_ingredients.append(iq)\n",
    "    return only_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b5c53b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ingredients_measures_quantity(ingredients_with_measures, ingredients_with_irregular_measures, ingredients_with_quantity, only_ingredients):\n",
    "    final_ingredients = {}\n",
    "    \n",
    "    # Agregar ingredientes con medidas y cantidades\n",
    "    for im in ingredients_with_measures.items():\n",
    "        ingredient_m = im[0]\n",
    "        quantity_m = im[1][0]\n",
    "        measure_m = im[1][1]\n",
    "        index_m = im[1][2]\n",
    "    \n",
    "        for iq in ingredients_with_quantity:\n",
    "            ingredient_q = iq[0]\n",
    "            ing_quantity = iq[1]\n",
    "        \n",
    "            # Ingredientes con cantidad y medidas\n",
    "            if ingredient_m == ingredient_q: \n",
    "                final_ingredients[ingredient_m] = [ quantity_m, measure_m, index_m ] #optimizar\n",
    "           \n",
    "    # Agregar ingredientes con medidas irregulares\n",
    "    for iim in ingredients_with_irregular_measures.items():\n",
    "        ingredient_iim = iim[0]\n",
    "     \n",
    "        if ingredient_iim in final_ingredients:\n",
    "            continue\n",
    "             \n",
    "        final_ingredients[ingredient_iim] = iim[1]\n",
    "        \n",
    "        \n",
    "    # Agregar solo ingredientes con cantidades\n",
    "    for iq in ingredients_with_quantity:\n",
    "        ingredient_q = iq[0]\n",
    "        quantity_q = iq[1]\n",
    "    \n",
    "        if ingredient_q in final_ingredients or quantity_q == '':\n",
    "            continue\n",
    "            \n",
    "        iq_copied = iq.copy()\n",
    "        iq_copied.remove(ingredient_q)\n",
    "        \n",
    "        final_ingredients[ingredient_q] = iq_copied\n",
    "    \n",
    "    # Agregar solo ingredientes\n",
    "    for oi in only_ingredients:\n",
    "        only_ing = oi[0]\n",
    "    \n",
    "        if only_ing not in final_ingredients:\n",
    "            oi_copied = oi.copy()\n",
    "            oi_copied.remove(only_ing)\n",
    "            final_ingredients[only_ing] = oi_copied \n",
    "\n",
    "    # Se ordenan por index en el texto\n",
    "    final_ingredients = dict(sorted(final_ingredients.items(), key=lambda item: item[1][2]))\n",
    "    return final_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "766e247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene adjetivos de los ingredientes\n",
    "\"\"\" \n",
    "def get_adjectives(tokenized_words, final_ingredients):\n",
    "    final_ingredients_adj = {}\n",
    "\n",
    "    for v_ingredient in final_ingredients.values():\n",
    "        adj = []\n",
    "        v_ingredient_idx = v_ingredient[2]\n",
    "    \n",
    "        ingredient = tokenized_words[v_ingredient_idx][1]\n",
    "    \n",
    "        for t_word in tokenized_words:\n",
    "            t_word_idx = t_word[0]\n",
    "            t_word_word = t_word[1]\n",
    "            t_word_tag = t_word[3]\n",
    "            t_word_det = t_word[4]\n",
    "    \n",
    "            if t_word_idx == v_ingredient_idx:\n",
    "                for idx in range(v_ingredient_idx + 1, len(tokenized_words)): # cambiar a infinito\n",
    "                    searched_adj = tokenized_words[idx]\n",
    "                    adj_word = searched_adj[1]\n",
    "                    adj_tag = searched_adj[3]\n",
    "                    adj_dep = searched_adj[5]\n",
    "                       \n",
    "                    if adj_dep == \"amod\" or adj_dep == \"nmod\" :\n",
    "                        adj.append(adj_word)\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "        final_ingredients_adj[ingredient] = adj\n",
    "    \n",
    "    return final_ingredients_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "32238000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene complementos de los ingredientes\n",
    "\"\"\" \n",
    "def get_complements(ngrams_ingredients, final_ingredients, final_ingredients_adj):\n",
    "    ingredients_complement = {}\n",
    "\n",
    "    for v_ingredient in final_ingredients:\n",
    "        complements_data = []\n",
    "    \n",
    "        v_ingredient_idx = final_ingredients[v_ingredient][2]\n",
    "        \n",
    "        for ngrams in ngrams_ingredients:\n",
    "            for ingredient, ingredient_data in ngrams.items():\n",
    "                complements = ingredient_data['complements']\n",
    "                ing_idx = ingredient_data['ing_idx']\n",
    "            \n",
    "                if v_ingredient_idx == ing_idx and len(complements) > 0:\n",
    "                    ingredients_complement[ingredient] = complements\n",
    "                \n",
    "        final_ingredients_adj.update(ingredients_complement)\n",
    "        \n",
    "    return final_ingredients_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "aa91f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene los ingredientes con toda su información correspondiente como diccionario.\n",
    "\"\"\"\n",
    "def merge_ingredients_data(final_ingredients, final_ingredients_adj):\n",
    "    total_ingredients = {}\n",
    "\n",
    "    for index, ingredient in enumerate(final_ingredients):\n",
    "        q_data_original = final_ingredients[ingredient].copy()\n",
    "        q_data = q_data_original[:-1]\n",
    "    \n",
    "        if ingredient not in final_ingredients_adj:\n",
    "            total_ingredients[ingredient] = { \"quantity\": q_data , \"description\": [] }\n",
    "            continue\n",
    "        \n",
    "        adj_data = final_ingredients_adj[ingredient]\n",
    "    \n",
    "        total_ingredients[ingredient] = { \"quantity\": q_data , \"description\": adj_data }\n",
    "    \n",
    "    return total_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "cf0cd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingredients():\n",
    "    # Se obtienen los ingredientes y las tres primeras columnas.\n",
    "    ingredients_set = get_ingredients_from_file('ingredientes.xlsx')\n",
    "\n",
    "    main_ingredient = ingredients_set.iloc[:, 0].str.lower()\n",
    "    complementary1  = ingredients_set.iloc[:, 1].str.lower()\n",
    "    complementary2  = ingredients_set.iloc[:, 2].str.lower()\n",
    "    \n",
    "    merge_ingredients_with_description(main_ingredient, complementary1, complementary2)\n",
    "    \n",
    "    # Se obtienen los datos de los datasets\n",
    "    return get_ingredients(main_ingredient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "6c2965b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ingredient(token):\n",
    "    ingr = ingredients()\n",
    "    return token.text.lower() in ingr is not False\n",
    "\n",
    "Token.set_extension('is_ingredient', getter=is_ingredient, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e26fe477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convierte los datos de los ingredientes a texto.\n",
    "\"\"\"\n",
    "def ingredients_data_to_string(data_ingredients):\n",
    "    ingredients_to_text = \"\"\n",
    "\n",
    "    for t_ingre in data_ingredients:\n",
    "        cantidad = data_ingredients[t_ingre]['quantity'][0]\n",
    "        medida = data_ingredients[t_ingre]['quantity'][1]\n",
    "        descripcion = ' '.join(data_ingredients[t_ingre]['description'])\n",
    "\n",
    "        if cantidad and medida:\n",
    "            medida += \" de\" \n",
    "\n",
    "        ingredient_array = [str(cantidad), medida, t_ingre, descripcion]\n",
    "        ingredient_string = \" \".join(ingredient_array)\n",
    "        ingredient_string = re.sub(' +',' ', ingredient_string)\n",
    "\n",
    "        ingredients_to_text += \"\\n\" + ingredient_string\n",
    "\n",
    "    return ingredients_to_text#, only_ingredients_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "66f99dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se inserta una \",\" entre las palabras del texto que corresponda.\n",
    "\"\"\"\n",
    "def insert_punct(tokenized_words, position):\n",
    "\n",
    "    # invertimos la lista de posiciones para agregar la \",\" de atras hacia adelante pra no alterar los ids de posiciones recibidos y ubicarlo en el lugar correcto\n",
    "    reverse_position = sorted(position, reverse=True)\n",
    "\n",
    "    #recorremos las posiciones y las buscamos en el texto, le sumamos una posicion y agregamos la \",\" y actualizamos el valor del tetxo de  que requiere punt en True\n",
    "    for pos in reverse_position:\n",
    "        tokenized_words[pos][7] = True\n",
    "        tokenized_words.insert(pos + 1, [pos + 1, ',', ',', 'PUNCT', 'punctuation', 'punct', '', False, [], []])\n",
    "\n",
    "    #actualizamos las properties correspondientes a cada palabra\n",
    "    text = [ token[1] for token in tokenized_words]\n",
    "    doc = nlp(\" \".join(text))\n",
    "    parsed_text = parse_text(doc, tokenized_words)\n",
    "\n",
    "    return get_tokenized_words(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ed0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se obtienen los ingredientes de un texto.\n",
    "\"\"\"\n",
    "def get_ingredients_from_text(parsed_text):\n",
    "\n",
    "    tokenized_words = get_tokenized_words(parsed_text)\n",
    "\n",
    "    words_text = get_word_text(tokenized_words)\n",
    "    \n",
    "    # Se obtienen los datos de los datasets\n",
    "    list_ingredients = ingredients()\n",
    "    measures_set = get_measures_from_file('medidas.json')\n",
    "    \n",
    "    # Medidas que nunca tienen cantidades\n",
    "    irregular_measures_set = parse_json_file('medidas irregulares.json')\n",
    "    \n",
    "    # Se inicializa la clase\n",
    "    wtn = WordsToInt()\n",
    "    \n",
    "    # Se obtienen los vecinos de los datos necesarios (ingredientes, cantidades y medidas)\n",
    "    ngrams_ingredients, tokenized_ingredients, position_punct = get_ngrams_ingredients(tokenized_words, words_text, list_ingredients)\n",
    "\n",
    "    tokenized_words = insert_punct(tokenized_words, position_punct)\n",
    "        \n",
    "    ngrams_numbers = get_ngrams_quantity(tokenized_words, words_text)\n",
    "    ngrams_measures = get_ngrams_measures(tokenized_words, words_text, measures_set)\n",
    "    \n",
    "    # Se obtienen los datos de los ingredientes encontrados\n",
    "    ingredients_with_measures = get_ingredients_with_measures(tokenized_words, ngrams_measures, list_ingredients, wtn)\n",
    "    ingredients_with_irregular_measures = get_ingredients_with_irregular_measures(irregular_measures_set, ngrams_ingredients)\n",
    "    ingredients_with_quantity= find_ingredients_with_quantity(ngrams_ingredients, tokenized_words, ingredients_with_measures, wtn)\n",
    "   \n",
    "    only_ingredients = find_only_ingredients(ingredients_with_quantity)\n",
    "    \n",
    "    # Se obtienen todos los ingredientes encontrados\n",
    "    final_ingredients = merge_ingredients_measures_quantity(ingredients_with_measures, ingredients_with_irregular_measures, ingredients_with_quantity, only_ingredients)\n",
    "\n",
    "    # Se obtienen los adjetivos y complementos\n",
    "    final_ingredients_adj = get_adjectives(tokenized_words, final_ingredients)\n",
    "    final_ingredients_adj = get_complements(ngrams_ingredients, final_ingredients, final_ingredients_adj)\n",
    "    \n",
    "    # Se unen las informaciones de los ingredientes encontrados.\n",
    "    data_ingredients = merge_ingredients_data(final_ingredients, final_ingredients_adj)\n",
    "\n",
    "    ingredients_data_to_string(data_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4572db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients():\n",
    "    textfile = os.getcwd() + \"/Converted_results/\" + \"Converted_audio.txt\"\n",
    "\n",
    "    try:\n",
    "        with open(textfile) as file:\n",
    "            fileContent = file.read()\n",
    "            file.close()\n",
    "    except FileNotFoundError:\n",
    "        print(f'The file {textfile} does not exist')\n",
    "    else:\n",
    "        doc = nlp(fileContent)\n",
    "        parsed_text = parse_text(doc)\n",
    "        from_description = get_text_from_description(parsed_text)\n",
    "\n",
    "        if len(from_description) == 0:\n",
    "            final_ingredients = get_ingredients_from_text(parsed_text)\n",
    "            print(final_ingredients)\n",
    "            return final_ingredients\n",
    "\n",
    "        else:\n",
    "            print(\" \".join(from_description))\n",
    "            return \" \".join(from_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
